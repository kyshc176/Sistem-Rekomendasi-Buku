# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13zhhNIjiL1ncYC62ikpQeIRSbnmKZ-BP

# Data Understanding

### Import libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tabulate import tabulate

"""Pada tahap awal ini, dilakukan proses import berbagai library yang digunakan dalam proyek ini, meliputi:

- **Pandas, NumPy**: Untuk manipulasi dan analisis data berbasis tabel (DataFrame) dan array numerik.
- **Matplotlib**: Untuk membuat visualisasi data dalam bentuk grafik atau plot.
- **TensorFlow, Keras**: Untuk membangun, melatih, dan menguji model deep learning menggunakan berbagai jenis layer.
- **Tabulate**: Untuk menampilkan output data dalam format tabel yang rapi dan mudah dibaca di console atau laporan.

### Load the dataset

Dataset dibaca menggunakan fungsi `pandas.read_csv()`, yang digunakan untuk memuat data dari file CSV ke dalam format DataFrame agar dapat dianalisis lebih lanjut.
"""

books = pd.read_csv("book-dataset/books.csv")
ratings = pd.read_csv("book-dataset/ratings.csv")
users = pd.read_csv("book-dataset/users.csv")

# dataset books
books.head()

# dataset ratings
ratings.head()

# dataset users
users.head()

print('Jumlah data buku:', len(books.ISBN.unique()))
print('Jumlah data rating buku dari pembaca:', len(ratings.ISBN.unique()))
print('jumlah data pengguna:', len(users['User-ID'].unique()))

"""Berdasarkan output yang ditampilkan, diperoleh informasi struktur dari tiga variabel utama dalam dataset:

### 1. `books` (271.360 data, 8 kolom)
Berisi informasi detail tentang buku:
- `ISBN`: Nomor identitas unik untuk setiap buku.
- `Book-Title`: Judul buku.
- `Book-Author`: Nama penulis buku.
- `Year-Of-Publication`: Tahun terbit buku.
- `Publisher`: Nama penerbit buku.
- `Image-URL-S`: URL gambar buku ukuran kecil.
- `Image-URL-M`: URL gambar buku ukuran sedang.
- `Image-URL-L`: URL gambar buku ukuran besar.

### 2. `ratings` (340.556 data, 3 kolom)
Mencatat penilaian pengguna terhadap buku:
- `User-ID`: Kode unik pengguna anonim.
- `ISBN`: Nomor identitas buku.
- `Book-Rating`: Nilai rating yang diberikan pengguna.

### 3. `users` (278.858 data, 3 kolom)
Menyimpan informasi pengguna:
- `User-ID`: Kode unik pengguna anonim.
- `Location`: Lokasi tempat tinggal pengguna.
- `Age`: Usia pengguna.

## Univariate Exploratory Data Analysis

Pada tahap ini dilakukan analisis eksploratif terhadap masing-masing variabel untuk memahami distribusi, karakteristik, serta pola awal yang ada pada data. Pemahaman ini akan membantu dalam memilih strategi dan algoritma yang tepat untuk proses rekomendasi buku.

### Variabel dalam Book Recommendation Dataset:
- **books**: Berisi informasi detail tentang buku, seperti judul, penulis, penerbit, tahun terbit, dan link gambar.
- **ratings**: Memuat data rating atau penilaian yang diberikan pengguna terhadap buku.
- **users**: Berisi informasi pengguna, termasuk lokasi dan usia sebagai bagian dari data demografis.

### Books Variabel

Langkah pertama dalam eksplorasi data adalah memeriksa isi awal dan struktur dari dataset `books`. Hal ini dilakukan untuk memastikan data berhasil dimuat dengan benar serta memahami tipe data pada setiap kolom.

### Langkah-langkah:
- Gunakan `head()` untuk melihat 5 baris pertama dari dataset.
- Gunakan `info()` untuk melihat jumlah entri, tipe data tiap kolom, dan nilai non-null pada dataset `books`.

Langkah ini penting untuk mengidentifikasi potensi masalah awal seperti missing value, tipe data yang tidak sesuai, atau struktur data yang perlu dibersihkan.
"""

# cek informasi dataset
books.info()

"""Berdasarkan output dari `books.info()`, diketahui bahwa dataset `books.csv` terdiri dari:

- **Jumlah entri**: 271.360 baris data
- **Jumlah kolom**: 8 kolom, yaitu:
  - `ISBN`
  - `Book-Title`
  - `Book-Author`
  - `Year-Of-Publication`
  - `Publisher`
  - `Image-URL-S`
  - `Image-URL-M`
  - `Image-URL-L`

## Top 10 Writer
"""

# Grouping'Book-Author' dan hitung jumlah buku yang ditulis oleh masing-masing penulis
author_counts = books.groupby('Book-Author')['Book-Title'].count()

# Urutkan penulis dalam urutan menurun
sorted_authors = author_counts.sort_values(ascending=False)

# Pilih 10 penulis teratas
top_10_authors = sorted_authors.head(10)

# Plot 10 penulis teratas dan buku yang ditulis oleh penulis kemudian dihitung menggunakan plot batang
plt.figure(figsize=(12, 6))
top_10_authors.plot(kind='bar')
plt.xlabel('Nama Penulis')
plt.ylabel('Jumlah Buku')
plt.title('10 Penulis Teratas Berdasarkan Jumlah Buku')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Dari data yang dianalisis, diketahui bahwa:

- Penulis **Agatha Christie** menulis buku terbanyak, yaitu lebih dari 600 judul.
- Dataset ini juga mengandung banyak penulis yang memiliki lebih dari satu judul buku.

Informasi ini penting untuk memahami distribusi kontribusi penulis dalam dataset dan dapat menjadi dasar dalam pengembangan sistem rekomendasi berbasis penulis.

### Ratings Variabel

Selanjutnya, dilakukan eksplorasi pada variabel `ratings` yang berisi penilaian buku dari pengguna. Data ini akan menjadi dasar dalam pengembangan model **collaborative filtering**.

Untuk memahami struktur dan kualitas data, gunakan fungsi `info()` untuk melihat jumlah entri, tipe data, dan nilai yang tidak kosong pada setiap kolom di dataset `ratings`.
"""

ratings.head()

ratings.info()

"""Berdasarkan output sebelumnya, dataset `ratings` memiliki:

- **Jumlah entri**: 1.149.780 baris
- **Kolom**: 3 kolom utama, yaitu:
  - `User-ID`: kode unik pengguna anonim
  - `ISBN`: nomor identitas unik buku
  - `Book-Rating`: nilai rating yang diberikan pengguna terhadap buku

"""

print('Jumlah User-ID:', len(ratings['User-ID'].unique()))
print('Jumlah buku berdasarkan ISBN:', len(ratings['ISBN'].unique()))

print('Jumlah rating buku:')
sorted_ratings = ratings['Book-Rating'].value_counts().sort_index()
pd.DataFrame({'Book-Rating': sorted_ratings.index, 'Jumlah': sorted_ratings.values})

"""Dari hasil analisis diketahui bahwa:

- Terdapat **105.283 pengguna unik** yang memberikan rating buku.
- Jumlah buku yang mendapat rating berdasarkan ISBN adalah **340.556 buku**.
- Rating yang diberikan memiliki rentang nilai dari **0 (paling rendah)** hingga **10 (paling tinggi)**.

Informasi ini penting sebagai dasar pemahaman skala rating untuk pengembangan model rekomendasi.

Dataset `ratings` memiliki lebih dari 1 juta baris, yang cukup besar dan memakan banyak memori saat pelatihan model.

Oleh karena itu, untuk menghemat memori dan mempercepat proses, hanya **5.000 data pertama** (baris indeks 0 hingga 4999) yang akan digunakan dalam pengembangan model collaborative filtering.

Dataset hasil pemangkasan ini kemudian disimpan dengan nama variabel baru `df_rating` agar tidak membingungkan dengan dataset asli.
"""

df_rating = ratings[:20000]
df_rating

"""### Users Variabel

Variabel `users` berisi informasi tentang pengguna anonim beserta data demografisnya.

Untuk memahami struktur data dan kualitas kolom, gunakan fungsi `info()` untuk melihat jumlah entri, tipe data, dan nilai non-null di setiap kolom.
"""

users.head()

users.info()

"""Dari output `users.info()`, diketahui bahwa dataset `users` memiliki:

- **278.858 entri**
- 3 kolom utama:
  - `User-ID`: kode unik pengguna anonim
  - `Location`: lokasi tempat tinggal pengguna
  - `Age`: usia pengguna (terdapat nilai yang tidak diketahui/bermasalah)

Dataset `users` berguna untuk pengembangan sistem rekomendasi berbasis demografi atau kondisi sosial pengguna. Namun, untuk studi kasus ini, data `users` **tidak akan digunakan** dalam model.

Model rekomendasi yang akan dikembangkan hanya menggunakan data dari **`books` dan `ratings`**.

## Data Preprocessing

### Menggabungkan Dataset dan Menghitung Jumlah Rating

Langkah selanjutnya adalah menggabungkan dataset `books` dan `ratings` berdasarkan kolom `ISBN`. Penggabungan ini berguna untuk mengaitkan informasi buku dengan rating yang diberikan oleh pengguna.

Setelah penggabungan, kita dapat menghitung jumlah total rating yang ada untuk analisis lebih lanjut.
"""

# Menggabungkan dataframe ratings dengan books berdasarkan nilai ISBN
books = pd.merge(ratings, books, on='ISBN', how='left')
books

"""Setelah penggabungan dataset `books` dan `ratings`, diperoleh dataset baru dengan **7 kolom** dan **1.149.780 baris** data.

Dataset ini menjadi dasar untuk pengembangan sistem rekomendasi buku.

Selanjutnya, dilakukan perhitungan jumlah rating yang diterima setiap buku berdasarkan `ISBN` menggunakan kode berikut:
"""

books.groupby('ISBN').sum()

"""# Data Preparation

Pada tahap ini dilakukan beberapa teknik penting untuk mempersiapkan data agar siap digunakan dalam pengembangan sistem rekomendasi berbasis konten, yaitu:

- **Menghilangkan missing value** untuk memastikan kualitas data yang bersih dan lengkap.
- **Menyamakan jenis buku berdasarkan ISBN**, karena pada sistem rekomendasi berbasis konten satu nomor ISBN harus mewakili satu judul buku yang unik.

Persiapan ini penting agar data valid dan konsisten selama proses pelatihan model.

Saat mencoba mengonversi kolom `Year-Of-Publication` ke tipe data integer, muncul error:

Hal ini menunjukkan bahwa terdapat data yang seharusnya berupa tahun, namun justru berisi teks. Setelah ditelusuri, ditemukan **dua nilai tidak valid**, yaitu:

- `'DK Publishing Inc'`
- `'Gallimard'`

Nilai-nilai ini merupakan **kesalahan input** dan perlu dihapus dari dataset agar tidak mengganggu proses konversi tipe data.
"""

books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

"""Menghapus value pada 'Year-Of-Publication' yang bernilai teks tersebut."""

temp = (books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')
books = books.drop(books[temp].index)
books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

"""Mengubah tipe data pada 'Year-Of-Publication'."""

books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
print(books.dtypes)

"""Setelah kolom `Year-Of-Publication` berhasil dikonversi ke tipe integer, selanjutnya dilakukan pembersihan data dengan menghapus kolom yang tidak relevan untuk pengembangan model rekomendasi berbasis konten.

Karena sistem rekomendasi akan fokus pada informasi buku berupa **judul** dan **penulis**, maka kolom yang berisi URL gambar (`Image-URL-S`, `Image-URL-M`, `Image-URL-L`) tidak diperlukan dan dapat dihapus untuk menyederhanakan dataset serta mengurangi beban komputasi.
"""

# Menghapus kolom Image-URL semua ukuran
books.drop(labels=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

books.head()

"""Setelah di hapus kolom Image-URL sekarang dataset hanya tersisa 5 kolom/variabel saja."""

print("Jumlah nomor ISBN Buku:", len(books['ISBN'].unique()))
print("Jumlah judul buku:", len(books['Book-Title'].unique()))
print('Jumlah penulis buku:', len(books['Book-Author'].unique()))
print('Jumlah Tahun Publikasi:', len(books['Year-Of-Publication'].unique()))
print('Jumlah nama penerbit:', len(books['Publisher'].unique()))

"""Berdasarkan output sebelumnya, diketahui:

- Jumlah **judul buku** unik: 242.135
- Jumlah **nomor ISBN** unik: 271.357

Hal ini mengindikasikan adanya buku yang **tidak memiliki nomor ISBN unik** atau terjadi duplikasi pada kolom ISBN, padahal setiap buku harus memiliki satu nomor ISBN yang unik.

### Tindakan selanjutnya:
Dataset akan difilter untuk memastikan setiap buku memiliki nomor ISBN yang valid dan unik, guna menjaga konsistensi data dalam proses pengembangan model rekomendasi.

### Mengatasi Missing Value
"""

# Cek missing value dengan fungsi isnull()
books.isnull().sum()

"""Dari analisis diketahui bahwa sebagian besar fitur memiliki missing value, kecuali kolom `User-ID`, `ISBN`, dan `Book-Rating` yang bersih tanpa missing value.

Kolom dengan missing value terbanyak adalah `Publisher` dengan 118.650 data kosong. Jumlah ini relatif kecil dibanding total data (1.149.780), sehingga penghapusan data dengan missing value dianggap layak.

Langkah selanjutnya adalah menghapus baris yang mengandung missing value dan menyimpan hasilnya dalam variabel baru bernama `all_books_clean`.
"""

all_books_clean = books.dropna()
all_books_clean

all_books_clean.isnull().sum()

"""Sekarang, dataset sudah bersih dan bisa lanjut ke tahap berikutnya.

### Menyamakan jenis buku berdasarkan ISBN

Sebelum tahap pemodelan, penting untuk memastikan bahwa setiap nomor ISBN hanya mewakili **satu judul buku** saja.

Jika terdapat nomor ISBN yang sama dengan judul berbeda, hal ini dapat menyebabkan bias dan inkonsistensi data.

Oleh karena itu, perlu dilakukan proses validasi dan penyamaan judul agar data menjadi konsisten dan siap untuk model rekomendasi.

Lakukan proses pengecekan ulang data setelah proses cleaning pada tahap sebelumnya. Buat variabel baru bernama fix_books untuk menyimpan dataframe.
"""

# Mengurutkan buku berdasarkan ISBN kemudian memasukkannya ke dalam variabel fix_books
fix_books = all_books_clean.sort_values('ISBN', ascending=True)
fix_books

# Mengecek berapa jumlah fix_books
len(fix_books['ISBN'].unique())

#cek jumlah judul buku
len(fix_books['Book-Title'].unique())

"""Diketahui terdapat nomor ISBN yang sama dengan judul buku berbeda, sehingga perlu dilakukan pembersihan data untuk memastikan setiap ISBN hanya muncul sekali.

Proses ini dilakukan dengan membuang data duplikat berdasarkan kolom `ISBN`, lalu hasilnya disimpan dalam variabel baru `preparation`.

Data yang sudah unik ini siap untuk tahap pemodelan selanjutnya.
"""

preparation = fix_books.drop_duplicates('ISBN')
preparation

"""Setelah data duplikat berdasarkan `ISBN` dihapus, lakukan pengecekan ulang jumlah data pada kolom:

- `ISBN`
- `Book-Title`
- `Book-Author`

Selanjutnya, konversi data dari tipe Series menjadi list menggunakan fungsi `tolist()`. Langkah ini memudahkan manipulasi dan analisis data selanjutnya.
"""

# konversi data series 'ISBN' menjadi bentuk list
isbn_id = preparation['ISBN'].tolist()

# konversi data series 'Book-Title' menjadi bentuk list
book_title = preparation['Book-Title'].tolist()

# konversi data series 'Book-Author' menjadi bentuk list
book_author = preparation['Book-Author'].tolist()

# konversi data series 'Year-Of-Publication' menjadi bentuk list
year_of_publication = preparation['Year-Of-Publication'].tolist()

# konversi data series 'Publisher' menjadi bentuk list
publisher = preparation['Publisher'].tolist()

print(len(isbn_id))
print(len(book_title))
print(len(book_author))
print(len(year_of_publication))
print(len(publisher))

"""Setelah memastikan data `ISBN`, `Book-Title`, `Book-Author`, `Year-Of-Publication`, dan `Publisher` sudah unik dan konsisten (tersisa 270.145 baris), langkah selanjutnya adalah membuat dictionary.

Dictionary ini berfungsi untuk menyimpan pasangan key-value dari masing-masing kolom, yang nantinya akan digunakan dalam proses pengembangan model sistem rekomendasi berbasis konten (content-based filtering).
"""

# Membuat dictionary untuk data ‘isbn_id’, ‘book_title’, ‘book_author’, 'year_of_publication', dan 'publisher'
books_new = pd.DataFrame({
    'isbn': isbn_id,
    'book_title': book_title,
    'book_author': book_author,
    'year_of_publication': year_of_publication,
    'publisher': publisher

})

books_new

"""Karena ukuran dataset yang sangat besar dapat menyebabkan penggunaan memori yang tinggi saat pelatihan model, pada proyek ini data yang digunakan akan dibatasi hanya pada **20.000 baris pertama** (exclude data ke-20.000).

Pembatasan ini bertujuan untuk menghemat memori dan mempercepat proses pengembangan model tanpa mengurangi kualitas hasil secara signifikan.
"""

books_new = books_new[:20000]

books_new

"""### Data Preparation Untuk Model Pengembangan dengan Collaborative Filtering

Sebelum membagi data menjadi training dan validasi, dilakukan beberapa langkah penting:

- Menyandikan (encode) fitur `User-ID` dan `ISBN` menjadi indeks integer agar model lebih mudah mengenali data.
- Membuat mapping `User-ID` dan `ISBN` ke dataframe yang berkaitan untuk referensi.
- Mengecek jumlah unik pengguna dan buku untuk memastikan data siap.
- Mengubah tipe data `Book-Rating` menjadi float agar sesuai untuk pelatihan model.

Langkah-langkah ini memudahkan proses transformasi data menjadi bentuk matriks numerik yang efisien untuk model collaborative filtering.
"""

# mengubah User-ID menjadi list tanpa nilai yang sama
user_ids = df_rating['User-ID'].unique().tolist()
print('list userID: ', user_ids)

# melakukan encoding User-ID
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('encoded userID: ', user_to_user_encoded)

# melakukan proses encoding angka ke User-ID
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
print('encoded angka ke userID: ', user_encoded_to_user)

# mengubah ISBN menjadi list tanpa nilai yang sama
isbn_id = df_rating['ISBN'].unique().tolist()

# melakukan encoding ISBN
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_id)}

# melakukan proses encoding angka ke ISBN
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_id)}

# Menonaktifkan peringatan SettingWithCopyWarning
pd.options.mode.chained_assignment = None  # "warn" atau "raise" untuk menyalakannya kembali

# Mapping User-ID ke dataframe user
df_rating['user'] = df_rating['User-ID'].map(user_to_user_encoded)

# Mapping ISBN ke dataframe judul buku
df_rating['book_title'] = df_rating['ISBN'].map(isbn_to_isbn_encoded)

"""Cek beberapa hal dalam data seperti jumlah user, jumlah judul buku, dan mengubah nilai rating menjadi float."""

# mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)

# mendapatkan jumlah judul buku
num_book_title = len(isbn_to_isbn_encoded)
print(num_book_title)

# mengubah rating menjadi nilai float
df_rating['Book-Rating'] = df_rating['Book-Rating'].values.astype(np.float32)

# nilai minimum rating
min_rating = min(df_rating['Book-Rating'])

# nilai maksimum rating
max_rating = max(df_rating['Book-Rating'])

print('Number of User: {}, Number of Book: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_book_title, min_rating, max_rating
))

"""Proses persiapan data telah selesai dilakukan. Data kini sudah dalam format yang sesuai dan siap digunakan untuk pembagian menjadi data training dan data validasi.

Langkah selanjutnya adalah melanjutkan ke proses pengembangan model collaborative filtering dengan menggunakan data yang telah dipersiapkan.

### Teknik yang digunakan:

- **TF-IDF Vectorizer**  
  Mengubah teks (judul buku) menjadi representasi vektor berdasarkan frekuensi kata penting.  
  - *Term Frequency (TF)*: frekuensi kata dalam dokumen.  
  - *Inverse Document Frequency (IDF)*: bobot unik kata dalam seluruh koleksi dokumen.

- **Cosine Similarity**  
  Mengukur kesamaan antar vektor fitur (judul buku) dengan menghitung sudut kosinus antara dua vektor.  
  Semakin kecil sudut, semakin tinggi tingkat kemiripan.

### TF-IDF Vectorizer

Import fungsi tfidfvectorizer() dari libray Sklearn.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data book_author
tf.fit(data['book_author'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Selanjutnya, lakukan fit dan transformasi ke dalam bentuk matriks."""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['book_author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Berdasarkan output, matriks TF-IDF memiliki ukuran `(20000, 8746)` dimana:

- **20000**: jumlah data buku yang digunakan.
- **8746**: jumlah fitur unik berupa kata dari nama penulis buku.

Untuk mengubah matriks TF-IDF yang bersifat sparse menjadi matriks dense (penuh), digunakan fungsi `todense()`.  
Matriks dense memudahkan pemrosesan lanjutan pada model rekomendasi.
"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Matriks TF-IDF hasil transformasi selanjutnya ditampilkan dalam bentuk DataFrame agar lebih mudah dipahami, dengan:

- Baris mewakili judul buku (Book-Title).
- Kolom mewakili nama penulis buku (Book-Author).

Format ini membantu memvisualisasikan hubungan bobot kata penulis pada setiap judul buku.
"""

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.book_title
).sample(15, axis=1).sample(10, axis=0)

"""Output matriks TF-IDF berhasil mengidentifikasi fitur penting dari judul buku berdasarkan nama penulis menggunakan fungsi `TfidfVectorizer`.

Pada contoh ini, hanya ditampilkan sampel acak dari matriks berupa:
- 10 judul buku (baris),
- 15 nama penulis buku (kolom),

sebagai representasi sebagian dari keseluruhan matriks yang berukuran 20.000 x 8.746.

### Cosine Similarity

Setelah mendapatkan representasi fitur penting dari judul buku menggunakan TF-IDF, langkah selanjutnya adalah menghitung derajat kesamaan antar judul buku.

Metode yang digunakan adalah **cosine similarity**, yang mengukur sudut kosinus antara dua vektor fitur, sehingga menilai tingkat kemiripan antar judul buku.

Nilai similarity ini akan digunakan dalam sistem rekomendasi berbasis konten untuk menemukan buku-buku yang paling relevan dengan preferensi pengguna.
"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Proses ini menghitung tingkat kemiripan antar judul buku berdasarkan representasi TF-IDF yang telah dibuat sebelumnya.

Dengan menggunakan fungsi `cosine_similarity` dari library `sklearn`, diperoleh matriks similarity yang berisi nilai kesamaan antar setiap pasangan judul buku.

Matriks similarity ini berbentuk array dua dimensi yang nantinya akan digunakan untuk memberikan rekomendasi buku berdasarkan kemiripan konten.
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_title'], columns=data['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap judul buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Dengan menggunakan cosine similarity, berhasil diidentifikasi tingkat kemiripan antar 20.000 judul buku. Matriks similarity yang dihasilkan berukuran (20.000 x 20.000), di mana masing-masing sumbu mewakili judul buku.

Karena ukuran matriks sangat besar dan tidak memungkinkan untuk menampilkan seluruh data, hanya sebagian kecil data yang ditampilkan, yaitu 10 judul buku pada sumbu vertikal dan 5 judul buku pada sumbu horizontal.

Data similarity ini nantinya akan digunakan untuk merekomendasikan daftar buku yang mirip dengan buku yang pernah dibaca atau dibeli oleh pengguna sebelumnya, sehingga sistem rekomendasi berbasis konten dapat memberikan rekomendasi yang relevan.

# Modeling

## Model Development dengan Content Based Filtering

Content-Based Filtering adalah metode rekomendasi yang memanfaatkan informasi atau "konten" dari item untuk memberikan rekomendasi.

- Ide utama: Mencocokkan preferensi pengguna dengan fitur dari item yang pernah disukai, misalnya buku dengan penulis yang sama.
- Contoh: Jika pengguna menyukai buku berjudul *Introduction to Machine Learning* karya *Alex Smola*, sistem merekomendasikan buku lain dengan penulis yang sama.

Pada proyek ini, digunakan fungsi `TfidfVectorizer()` dan `cosine_similarity()` dari library Scikit-learn.

Sebelum memulai pengembangan model Content-Based Filtering, dilakukan pengecekan ulang terhadap dataset yang sudah disiapkan.

Selanjutnya, dataframe hasil persiapan data pada tahap sebelumnya di-assign ke variabel `data` agar siap digunakan pada proses pemodelan.
"""

data = books_new
data.sample(5)

"""### Mendapatkan Rekomendasi

Pada langkah ini, kita akan membuat sebuah fungsi bernama book_recommendations dengan parameter sebagai berikut:

- book_title: judul buku yang akan dijadikan acuan untuk mencari rekomendasi (sesuai dengan index pada dataframe similarity).
- similarity_data: dataframe yang berisi data similarity (kemiripan) antar judul buku yang sudah dihitung sebelumnya.
- items: dataframe yang memuat informasi buku, khususnya kolom judul buku (book_title) dan nama penulis (book_author), yang akan digunakan untuk menampilkan hasil rekomendasi.
- k: jumlah rekomendasi buku yang ingin ditampilkan, dengan nilai default 5.
"""

def book_recommendation(book_title, similarity_data=cosine_sim_df, items=data[['book_title', 'book_author']], k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop book_title agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Perlu diperhatikan bahwa dengan menggunakan fungsi argpartition, kita dapat mengambil sejumlah nilai tertinggi sebanyak k dari data similarity (dalam kasus ini adalah dataframe cosine_sim_df). Selanjutnya, hasil nilai tertinggi tersebut diurutkan dari bobot kemiripan yang paling besar hingga terkecil, dan disimpan dalam variabel closest.

Kemudian, langkah penting berikutnya adalah menghapus judul buku yang sedang dicari (book_title) dari daftar rekomendasi agar buku tersebut tidak muncul kembali dalam hasil rekomendasi. Hal ini dilakukan karena tujuan sistem adalah mencari buku lain yang serupa dengan buku yang diinput, bukan menampilkan buku itu sendiri.

Gunakan fungsi book_recommendation tersebut untuk membuat rekomendasi 5 buku teratas yang direkomendasikan oleh sistem.
"""

book_title_test = "Entering the Silence : Becoming a Monk and a Writer (The Journals of Thomas Merton, V. 2)" # contoh judul buku

data[data.book_title.eq(book_title_test)]

"""Perhatikan bahwa buku berjudul Entering the Silence: Becoming a Monk and a Writer (The Journals of Thomas Merton, V. 2) ditulis oleh Thomas Merton. Sekarang, tolong gunakan fungsi book_recommendation untuk mendapatkan rekomendasi buku yang terkait dengan judul tersebut."""

# Mendapatkan rekomendasi judul buku yang mirip
book_recommendation(book_title_test)

"""Berdasarkan output diatas, sistem berhasil merekomendasikan 5 judul buku teratas dengan kategori nama penulis (book_author) yaitu 'Thomas Merton'.

## Model Development dengan Collaborative Filtering

Content-Based Filtering adalah metode rekomendasi yang memanfaatkan informasi dari konten item untuk memberikan rekomendasi.

- **Ide Utama:** mencocokkan preferensi pengguna dengan fitur item yang pernah disukai, misalnya buku dengan penulis yang sama.  
- **Contoh:** Jika pengguna suka buku *Introduction to Machine Learning* karya *Alex Smola*, sistem merekomendasikan buku lain dari penulis tersebut.

## Teknik yang dipakai:

- **TF-IDF Vectorizer**  
  Mengubah teks (judul buku) jadi vektor berdasarkan frekuensi kata penting.  

- **Cosine Similarity**  
  Mengukur kemiripan antar vektor dengan menghitung sudut kosinusnya; makin kecil sudut, makin mirip.

Di proyek ini, digunakan fungsi `TfidfVectorizer()` dan `cosine_similarity()` dari Scikit-learn.

### Membagi data untuk Training dan Validasi

Sebelum dilakukan pembagian data menjadi training dan validasi, data terlebih dahulu diacak agar distribusinya menjadi random.
"""

# mengacak dataset
df_rating = df_rating.sample(frac=1, random_state=42)
df_rating

"""Selanjutnya, dilakukan proses pembagian data menjadi data train dan validasi dengan komposisi 90:10. Namun sebelumnya, perlu dipetakan (mapping) data user dan judul buku menjadi satu value terlebih dahulu. Kemudian, dibuat rating dalam skala 0 sampai 1 agar mudah dalam melakukan proses training."""

# membuat variabel x untuk mencocokkan data user dan judul buku menjadi satu value
x = df_rating[['user', 'book_title']].values

# membuat variabel y untuk membuat rating dari hasil
y = df_rating['Book-Rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# membagi menjadi 90% data train dan 10% data validasi

train_indices = int(0.9 * df_rating.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""Data sudah siap untuk dimasukkan ke dalam pengembangan model dengan collaborative filtering.

### Proses training

Pada proses training, model menghitung skor kecocokan antara pengguna dan judul buku menggunakan teknik embedding.  

Langkah-langkahnya:  
1. Embedding dilakukan pada data **user** dan **book_title**.  
2. Hitung dot product antara embedding user dan book_title.  
3. Tambahkan bias untuk masing-masing user dan book_title.  
4. Skor kecocokan dipetakan ke rentang [0, 1] menggunakan fungsi aktivasi **sigmoid**.

Model dibuat menggunakan class `RecommenderNet` yang merupakan turunan dari *Keras Model class*. Kode ini terinspirasi dari tutorial di situs *Keras* dengan beberapa penyesuaian layer sesuai kebutuhan proyek.
"""

class RecommenderNet(tf.keras.Model):

    # inisialisasi fungsi
    def __init__(self, num_users, num_book_title, embedding_size, dropout_rate=0.2, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_book_title = num_book_title
        self.embedding_size = embedding_size
        self.dropout_rate = dropout_rate

        self.user_embedding = layers.Embedding( # layer embedding user
            num_users,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias

        self.book_title_embedding = layers.Embedding( # layer embedding book_title
            num_book_title,
            embedding_size,
            embeddings_initializer = 'he_normal',
            embeddings_regularizer = keras.regularizers.l2(1e-6)
        )
        self.book_title_bias = layers.Embedding(num_book_title, 1) # layer embedding book_title

        self.dropout = layers.Dropout(rate=dropout_rate)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0]) # memanggil layer embedding 1
        user_vector = self.dropout(user_vector)
        user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2

        book_title_vector = self.book_title_embedding(inputs[:, 1]) # memanggil layer embedding 3
        book_title_vector = self.dropout(book_title_vector)
        book_title_bias = self.book_title_bias(inputs[:, 1]) # memanggil layer embedding 4

        dot_user_book_title = tf.tensordot(user_vector, book_title_vector, 2) # perkalian dot product

        x = dot_user_book_title + user_bias + book_title_bias

        return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_book_title, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=1e-4),
    metrics = [tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini menggunakan:  
- **Binary Crossentropy** sebagai fungsi loss,  
- **Adam (Adaptive Moment Estimation)** sebagai optimizer,  
- dan **Root Mean Squared Error (RMSE)** sebagai metrik evaluasi
"""

# memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 16,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""Berdasarkan hasil training, model menunjukkan performa yang cukup baik dan konvergen pada sekitar 50 epochs.  
Didapatkan nilai Root Mean Squared Error (RMSE) sekitar **0.2939** pada data training dan **0.3353** pada data validasi.  

Nilai ini menunjukkan model sudah cukup baik untuk digunakan dalam sistem rekomendasi.  
Langkah selanjutnya adalah menggunakan model untuk menghasilkan rekomendasi judul buku sesuai preferensi pengguna.

### Mendapatkan Rekomendasi Judul Buku

Untuk mendapatkan rekomendasi judul buku:  
1. Ambil sampel user secara acak.  
2. Definisikan variabel `book_not_readed`, yaitu daftar buku yang belum pernah dibaca atau dibeli oleh pengguna. Variabel ini menjadi kandidat judul buku yang akan direkomendasikan.  
3. Variabel `book_not_readed` diperoleh dengan menggunakan operator bitwise `~` pada variabel `book_readed_by_user`.
"""

book_df = books_new

# mengambil sampel user
user_id = df_rating['User-ID'].sample(1).iloc[0]
book_readed_by_user = df_rating[df_rating['User-ID'] == user_id]

# membuat variabel book_not_readed
book_not_readed = book_df[~book_df['isbn'].isin(book_readed_by_user['ISBN'].values)]['isbn']
book_not_readed = list(
    set(book_not_readed)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_readed = [[isbn_to_isbn_encoded.get(x)] for x in book_not_readed]
user_encoder = user_to_user_encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_not_readed), book_not_readed)
)

"""Gunakan fungsi model.predict() dari library Keras, untuk memperoleh rekomendasi judul buku"""

ratings_model = model.predict(user_book_array).flatten()

top_ratings_indices = ratings_model.argsort()[-10:][::-1]

recommended_book_ids = [
    isbn_encoded_to_isbn.get(book_not_readed[x][0]) for x in top_ratings_indices
]

top_book_user = (
    book_readed_by_user.sort_values(
        by='Book-Rating',
        ascending=False
    )
    .head(10)['ISBN'].values
)

book_df_rows = book_df[book_df['isbn'].isin(top_book_user)]

# Menampilkan rekomendasi buku dalam bentuk DataFrame
book_df_rows_data = []
for row in book_df_rows.itertuples():
    book_df_rows_data.append([row.book_title, row.book_author])

recommended_book = book_df[book_df['isbn'].isin(recommended_book_ids)]

recommended_book_data = []
for row in recommended_book.itertuples():
    recommended_book_data.append([row.book_title, row.book_author])

# Membuat DataFrame untuk output
output_columns = ['Book Title', 'Book Author']
df_book_readed_by_user = pd.DataFrame(book_df_rows_data, columns=output_columns)
df_recommended_books = pd.DataFrame(recommended_book_data, columns=output_columns)

# Menampilkan hasil rekomendasi dalam bentuk DataFrame
print("Showing recommendation for users: {}".format(user_id))
print("===" * 9)
print("Book with high ratings from user")
print("----" * 8)
print(df_book_readed_by_user)
print("----" * 8)
print("Top 10 books recommendation")
print("----" * 8)
df_recommended_books

"""Berdasarkan output di atas, rekomendasi untuk user dengan ID 1235 berhasil dibuat.  

Dari hasil tersebut, dapat dibandingkan antara:  
- **Book with high ratings from user**  
- **Top 10 books recommendation**  

Beberapa judul buku rekomendasi juga menyertakan nama penulis yang sesuai dengan preferensi user.  

Didapatkan 10 buku rekomendasi teratas lengkap dengan nama penulis, serta 1 judul buku dengan rating tertinggi dari user tersebut.

# Evaluation

## Evaluasi Model dengan Content Based Filtering

Metrik yang digunakan dalam proyek ini adalah:  
- **Precision**: rasio item relevan yang direkomendasikan terhadap total item yang direkomendasikan.  
- **Recall**: rasio item relevan yang direkomendasikan terhadap total item relevan yang seharusnya direkomendasikan.  
- **F1-Score**: harmonisasi antara Precision dan Recall dalam satu nilai.

Sebelum menghitung metrik tersebut, diperlukan data **ground truth** yang berisi label sebenarnya untuk menilai hasil prediksi model.  

Pada proyek ini, data ground truth dibuat berdasarkan derajat kemiripan antar judul buku yang dihitung dengan **cosine similarity**.  
- Setiap baris dan kolom mewakili judul buku.  
- Nilai pada setiap sel adalah label:  
  - `1` = similar  
  - `0` = tidak similar  

Ditetapkan juga sebuah **threshold** untuk menentukan apakah nilai similarity antara dua buku dianggap 1 (similar) atau 0 (tidak similar).
"""

# Menentukan threshold untuk mengkategorikan similarity sebagai 1 atau 0
threshold = 0.5

# Membuat ground truth data dengan asumsi threshold
ground_truth = np.where(cosine_sim >= threshold, 1, 0)

# Menampilkan beberapa nilai pada ground truth matrix
ground_truth_df = pd.DataFrame(ground_truth, index=data['book_title'], columns=data['book_title']).sample(5, axis=1).sample(10, axis=0)

"""Pada kode di atas, nilai ambang batas (threshold) ditetapkan sebesar **0.5**.  
Nilai threshold ini disesuaikan berdasarkan kebutuhan dan karakteristik hasil rekomendasi sebelumnya.  

Kemudian, matriks ground truth dibuat menggunakan fungsi `np.where()` dari NumPy, dimana:  
- Posisi dengan nilai cosine similarity ≥ threshold diberi nilai **1** (similar).  
- Posisi dengan nilai similarity < threshold diberi nilai **0** (tidak similar).  

Matriks hasil tersebut kemudian disajikan dalam bentuk **DataFrame**, dengan baris dan kolom yang diindeks oleh judul buku dari dataset.

Berikut contoh tampilan DataFrame ground truth.

"""

ground_truth_df

"""Setelah matriks ground truth dibuat, langkah berikutnya adalah menghitung evaluasi model menggunakan metrik **precision**, **recall**, dan **f1 score**.

- Fungsi `precision_recall_fscore_support` dari library Sklearn digunakan untuk menghitung ketiga metrik tersebut.  
- Karena keterbatasan memori, hanya diambil sekitar 10.000 sampel dari matriks cosine similarity dan ground truth untuk mempercepat perhitungan.  
- Matriks cosine similarity dan ground truth kemudian diubah menjadi array satu dimensi untuk memudahkan perbandingan dan perhitungan.  
- Nilai cosine similarity dikategorikan berdasarkan threshold:  
  - ≥ threshold → 1 (positif)  
  - < threshold → 0 (negatif)  
- Hasil prediksi disimpan dalam array `predictions`.  
- Fungsi `precision_recall_fscore_support` dipanggil dengan parameter `average='binary'` (karena klasifikasi biner) dan `zero_division=1` (menghindari error pembagian nol).

Dengan cara ini, diperoleh nilai precision, recall, dan f1 score sebagai ukuran performa model rekomendasi.

"""

from sklearn.metrics import precision_recall_fscore_support

# Mengambil sebagian kecil dari cosine similarity matrix dan ground truth matrix
sample_size = 10000
cosine_sim_sample = cosine_sim[:sample_size, :sample_size]
ground_truth_sample = ground_truth[:sample_size, :sample_size]

# Mengonversi cosine similarity matrix menjadi array satu dimensi untuk perbandingan
cosine_sim_flat = cosine_sim_sample.flatten()

# Mengonversi ground truth matrix menjadi array satu dimensi
ground_truth_flat = ground_truth_sample.flatten()

# Menghitung metrik evaluasi
predictions = (cosine_sim_flat >= threshold).astype(int)
precision, recall, f1, _ = precision_recall_fscore_support(
    ground_truth_flat, predictions, average='binary', zero_division=1
)

print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)

"""Berdasarkan evaluasi model dengan metrik precision, recall, dan F1 Score didapatkan hasil sebagai berikut:  

- **Precision:** 1.0  
  Menunjukkan bahwa semua prediksi positif model benar tanpa adanya false positive.  

- **Recall:** 1.0  
  Model berhasil mengidentifikasi 100% item yang sebenarnya relevan.  

- **F1 Score:** ~1.0  
  Menunjukkan keseimbangan yang sangat baik antara precision dan recall.  

**Kesimpulan:**  
Model content-based filtering ini bekerja dengan sangat baik dalam memberikan rekomendasi item yang relevan.

## Evaluasi Model dengan Collaborative Filtering

Metrik evaluasi yang digunakan adalah **Root Mean Squared Error (RMSE)**.  
RMSE umum dipakai untuk mengukur seberapa baik model memprediksi nilai kontinu dengan membandingkan hasil prediksi dengan nilai sebenarnya.  

Dalam konteks collaborative filtering, RMSE mengukur seberapa akurat model memprediksi preferensi pengguna terhadap item.  

Berdasarkan hasil training, diperoleh nilai RMSE untuk data training dan validasi.  
Untuk memvisualisasikan proses training dan evaluasi model, digunakan plotting metrik dengan library **matplotlib**.  
Kode plotting dapat diterapkan sebagai berikut:
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""

Dari visualisasi metrik RMSE pada model, terlihat bahwa model konvergen sekitar **epochs ke-50**.  

Plot metrik menunjukkan nilai error (MSE) yang cukup kecil.  

Nilai RMSE akhir yang diperoleh:  
- **Training:** 0.2939  
- **Validasi:** 0.3353  

Nilai RMSE yang kecil ini menunjukkan model mampu memprediksi preferensi pengguna terhadap item dengan baik.  

Sehingga, rekomendasi yang dihasilkan oleh model cukup akurat dan dapat diandalkan.
"""